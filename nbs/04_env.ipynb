{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "> Text Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List, Callable, Tuple, Iterable\n",
    "\n",
    "import torch\n",
    "from torchtyping import TensorType\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextEnv(gym.Env):\n",
    "    def __init__(\n",
    "        self, model: Callable, tokenizer: Callable,\n",
    "        observation_input: List[int], context_length: int=1024\n",
    "    ):\n",
    "        \"\"\"Initialize the environment.\n",
    "\n",
    "        Args:\n",
    "            model (_type_): The RL-based language model\n",
    "            tokenizer (_type_): The tokenizer of the RL-based language model\n",
    "            observation_input (list, optional): The list of all possible prompts.\n",
    "            context_length (int, optional): The max context length of the RL-based language model . Defaults to 1024.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "        self.context_length = context_length\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        vocab_ids = [token_id for text, token_id in tokenizer.vocab.items()]\n",
    "        self.action_space = gym.spaces.Discrete(len(vocab_ids))\n",
    "        self.actions = vocab_ids\n",
    "        \n",
    "        # TODO: add support a batch of observation space\n",
    "        # currently only support one prompt\n",
    "        self.observation_input = observation_input\n",
    "        self.input_token_ids: List[int] = []\n",
    "        self.predicted_token_ids: List[int] = []\n",
    "    \n",
    "    def is_in_action_space(self, action: int) -> bool:\n",
    "        return True if action in self.actions else False\n",
    "\n",
    "    def is_token_end(self, token_id: int) -> bool:\n",
    "        end_tokens = [self.tokenizer.eos_token_id, self.tokenizer.sep_token_id]\n",
    "        return True if token_id in end_tokens else False\n",
    "\n",
    "    def is_max_context_length(self, tokens) -> bool:\n",
    "        n_tokens = len(tokens)\n",
    "        return True if n_tokens >= self.context_length else False\n",
    "\n",
    "    def _add_predicted_token(self, token_id: int):\n",
    "        \"\"\"Add the token_id to the list of current predicted token\n",
    "\n",
    "        Args:\n",
    "            token_id (int): A token\n",
    "        \"\"\"\n",
    "        self.predicted_token_ids.append(token_id)\n",
    "        \n",
    "    def step(self, action: int) -> Tuple[\n",
    "        TensorType[\"seq_len\", \"n_embd\"],\n",
    "        int, bool, bool, dict, bool\n",
    "    ]:\n",
    "        assert self.is_in_action_space(action) == True\n",
    "        \n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        if self.is_token_end(action) or self.is_max_context_length(self.input_token_ids):\n",
    "            done = True\n",
    "            reward = self._get_reward(self.predicted_token_ids)\n",
    "        else:\n",
    "            done = False\n",
    "            reward = 0\n",
    "            self._add_predicted_token(action)\n",
    "        \n",
    "        next_observation = self._get_obs()\n",
    "        \n",
    "        return next_observation, reward, terminated, truncated, info, done\n",
    "    \n",
    "    def _get_reward(self, action: int):\n",
    "        raise Exception(\"Write your own reward function!\")\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"Return the current observation of the environment.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        input_token_ids = self.input_token_ids\n",
    "        predicted_token_ids = self.predicted_token_ids\n",
    "        \n",
    "        if isinstance(self.input_token_ids, torch.Tensor) is False:\n",
    "            input_token_ids = torch.tensor(self.input_token_ids, dtype=torch.int)\n",
    "            \n",
    "        if isinstance(self.predicted_token_ids, torch.Tensor) is False:\n",
    "            predicted_token_ids = torch.tensor(self.predicted_token_ids, dtype=torch.int)\n",
    "        \n",
    "        # `transformer` do inference in batch, so add an extra dim for batch\n",
    "        input_ids = torch.cat([input_token_ids, predicted_token_ids]).unsqueeze(0)\n",
    "        \n",
    "        # output = self.model(\n",
    "        #     input_ids=input_ids,\n",
    "        #     output_hidden_states=True,\n",
    "        # )\n",
    "        # last_hidden_state = output.hidden_states[-1]\n",
    "        # return last_hidden_state\n",
    "    \n",
    "        return input_ids\n",
    "    \n",
    "    def reset(self) -> TensorType[\"seq_len\"]:\n",
    "        # the current input prompts's token_ids\n",
    "        self.input_token_ids: List[int] = self.tokenizer(self.observation_input, return_tensors=\"pt\")[\"input_ids\"].tolist()[0]\n",
    "\n",
    "        # the current generated token_ids\n",
    "        self.predicted_token_ids: List[int] = []\n",
    "\n",
    "        return self._get_obs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
