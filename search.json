[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "InstructGoose",
    "section": "",
    "text": "Paper: InstructGPT - Training language models to follow instructions with human feedback"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "InstructGoose",
    "section": "Install",
    "text": "Install\nInstall from PipPy\npip install instruct-goose\nInstall directly from the source code\ngit clone https://github.com/xrsrke/instructGOOSE.git\ncd instructGOOSE\npip install -e ."
  },
  {
    "objectID": "index.html#train-the-rl-based-language-model",
    "href": "index.html#train-the-rl-based-language-model",
    "title": "InstructGoose",
    "section": "Train the RL-based language model",
    "text": "Train the RL-based language model\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import optim\n\nfrom instruct_goose import Agent, RewardModel, RLHFTrainer, RLHFConfig, create_reference_model\nStep 1: Load dataset\ndataset = load_dataset(\"imdb\", split=\"train\")\ntrain_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\nStep 2: Load the pre-trained model and tokenizer\nmodel_base = AutoModelForCausalLM.from_pretrained(\"gpt2\")\nreward_model = RewardModel(\"gpt2\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\neos_token_id = tokenizer.eos_token_id\ntokenizer.pad_token = tokenizer.eos_token\nStep 3: Create the RL-based language model agent and the reference model\nmodel = Agent(model_base)\nref_model = create_reference_model(model)\nStep 4: Train it\nmax_new_tokens = 20\ngeneration_kwargs = {\n    \"min_length\":-1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n    \"max_new_tokens\": max_new_tokens\n}\n\nconfig = RLHFConfig()\nN_EPOCH = 100\ntrainer = RLHFTrainer(model, ref_model, config)\noptimizer = optim.SGD(model.parameters(), lr=1e-3)\nfor epoch in range(N_EPOCH):\n    for batch in train_dataloader:\n        inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, return_tensors=\"pt\")\n        response_ids = model.generate(\n            inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"],\n            **generation_kwargs\n        )\n        \n        # extract the generated text\n        response_ids = response_ids[:, -max_new_tokens:]\n        response_attention_mask = torch.ones_like(response_ids)\n        \n        # evaluate from the reward model\n        with torch.no_grad():\n            text_input_ids = torch.stack([torch.concat([q, r]) for q, r in zip(inputs[\"input_ids\"], response_ids)], dim=0)\n            rewards = reward_model(text_input_ids)\n        \n        # calculate PPO loss\n        loss = trainer.compute_loss(\n            query_ids=inputs[\"input_ids\"],\n            query_attention_mask=inputs[\"attention_mask\"],\n            response_ids=response_ids,\n            response_attention_mask=response_attention_mask,\n            rewards=rewards\n        )\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        print(f\"loss={loss}\")"
  },
  {
    "objectID": "index.html#todo",
    "href": "index.html#todo",
    "title": "InstructGoose",
    "section": "TODO",
    "text": "TODO\n\nAdd support custom reward function\nAdd support custom value function\nAdd support non-transformer models"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "InstructGoose",
    "section": "Resources",
    "text": "Resources\nI implemented this using these resources\n\nCopied the load_yaml function from https://github.com/Dahoas/reward-modeling\nHow to build a dataset to train reward model: https://wandb.ai/carperai/summarize_RLHF/reports/Implementing-RLHF-Learning-to-Summarize-with-trlX–VmlldzozMzAwODM2\nHow to add value head in PPO agent: https://github.com/lvwerra/trl\nHow to calculate the loss of PPO agent: https://github.com/lvwerra/trl/blob/main/trl/trainer/ppo_trainer.py\nHow to use PPO to train RLHF agent: https://github.com/voidful/TextRL\nHow PPO works: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py\nCopied the compute advantages and returns from TLR: https://github.com/lvwerra/trl/blob/d2e8bcf8373726fb92d2110c500f7df6d0bd566d/trl/trainer/ppo_trainer.py#L686"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utility functions",
    "section": "",
    "text": "source\n\nload_yaml\n\n load_yaml (config_path)\n\n\nsource\n\n\nRLHFConfig\n\n RLHFConfig (epsilon:float=0.1, ent_coef:float=0.01, vf_coef:float=0.1)\n\n\n\nReference Model\n\nsource\n\n\ncreate_reference_model\n\n create_reference_model (model)"
  },
  {
    "objectID": "trainer.html",
    "href": "trainer.html",
    "title": "Trainer",
    "section": "",
    "text": "\\(L_t^{C L I P+V F+S}(\\theta)=\\hat{\\mathbb{E}}_t\\left[L_t^{C L I P}(\\theta)-c_1 L_t^{V F}(\\theta)+c_2 S\\left[\\pi_\\theta\\right]\\left(s_t\\right)\\right]\\)\n\\(L^{C L I P}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\min \\left(r_t(\\theta) \\hat{A}_t, \\operatorname{clip}\\left(r_t(\\theta), 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t\\right)\\right]\\)\n\\(\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)} = \\log(\\pi_\\theta\\left(a_t \\mid s_t\\right)) - \\log(\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right))\\)\n\\(r_t(\\theta)=\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)}\\)\n\nsource\n\nRLHFTrainer\n\n RLHFTrainer (model:Callable, ref_model:Callable, config)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDetails\n\n\n\n\nmodel\ntyping.Callable\nA pre-trained language model\n\n\nref_model\ntyping.Callable\nA a reference model\n\n\nconfig\n\n\n\n\n\n\nsource\n\n\nRLHFTrainer.compute_loss\n\n RLHFTrainer.compute_loss (query_ids:typing.Annotated[torch.Tensor,{'__tor\n                           chtyping__':True,'details':('batch_size','seq_l\n                           en',),'cls_name':'TensorType'}], query_attentio\n                           n_mask:typing.Annotated[torch.Tensor,{'__torcht\n                           yping__':True,'details':('batch_size','seq_len'\n                           ,),'cls_name':'TensorType'}], response_ids:typi\n                           ng.Annotated[torch.Tensor,{'__torchtyping__':Tr\n                           ue,'details':('batch_size','seq_len',),'cls_nam\n                           e':'TensorType'}], response_attention_mask:typi\n                           ng.Annotated[torch.Tensor,{'__torchtyping__':Tr\n                           ue,'details':('batch_size','seq_len',),'cls_nam\n                           e':'TensorType'}], rewards:typing.Annotated[tor\n                           ch.Tensor,{'__torchtyping__':True,'details':('b\n                           atch_size',),'cls_name':'TensorType'}])\n\nCalculate PPO’s loss."
  },
  {
    "objectID": "reward_model.html",
    "href": "reward_model.html",
    "title": "Reward Model",
    "section": "",
    "text": "Reward Model\n\nsource\n\n\nRewardModel\n\n RewardModel (checkpoint:str, dropout:float=0.1)\n\nReward model.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncheckpoint\nstr\n\ntransformers’s model path\n\n\ndropout\nfloat\n0.1\n\n\n\n\n\nsource\n\n\nRewardModel.forward\n\n RewardModel.forward (input_ids:typing.Annotated[torch.Tensor,{'__torchtyp\n                      ing__':True,'details':('batch_size','seq_len',),'cls\n                      _name':'TensorType'}], attention_mask:typing.Annotat\n                      ed[torch.Tensor,{'__torchtyping__':True,'details':('\n                      batch_size','seq_len',),'cls_name':'TensorType'}]=No\n                      ne)\n\nCalculate reward for each item in a batch.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_ids\ntyping.Annotated[torch.Tensor, {‘torchtyping’: True, ‘details’: (‘batch_size’, ‘seq_len’,), ‘cls_name’: ‘TensorType’}]\n\n\n\n\nattention_mask\ntyping.Annotated[torch.Tensor, {‘torchtyping’: True, ‘details’: (‘batch_size’, ‘seq_len’,), ‘cls_name’: ‘TensorType’}]\nNone\n\n\n\nReturns\ntyping.Annotated[torch.Tensor, {‘torchtyping’: True, ‘details’: (‘batch_size’, 1,), ‘cls_name’: ‘TensorType’}]\n\nA reward scalar for each item in a batch\n\n\n\n\n\nPairwise Loss\n\\(\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c} K \\\\ 2 \\end{array}\\right)} E_{\\left(x, y_w, y_l\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\right)\\right]\\)\n\nsource\n\n\nPairwiseLoss\n\n PairwiseLoss ()\n\nPairwise loss function.\n\nsource\n\n\nPairwiseLoss.forward\n\n PairwiseLoss.forward (chosen_rewards:typing.Annotated[torch.Tensor,{'__to\n                       rchtyping__':True,'details':('batch_size',1,),'cls_\n                       name':'TensorType'}], rejected_rewards:typing.Annot\n                       ated[torch.Tensor,{'__torchtyping__':True,'details'\n                       :('batch_size',1,),'cls_name':'TensorType'}])\n\nForward pass.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nchosen_rewards\ntyping.Annotated[torch.Tensor, {‘torchtyping’: True, ‘details’: (‘batch_size’, 1,), ‘cls_name’: ‘TensorType’}]\nThe reward of the chosen prompt\n\n\nrejected_rewards\ntyping.Annotated[torch.Tensor, {‘torchtyping’: True, ‘details’: (‘batch_size’, 1,), ‘cls_name’: ‘TensorType’}]\nThe reward of the rejected prompt\n\n\nReturns\ntyping.Annotated[torch.Tensor, {‘torchtyping’: True, ‘details’: (1,), ‘cls_name’: ‘TensorType’}]\nA scalar loss"
  },
  {
    "objectID": "agent.html",
    "href": "agent.html",
    "title": "Agent",
    "section": "",
    "text": "Agent (The RL-based language model)\n\nsource\n\n\nAgent\n\n Agent (model:Callable)\n\nThe RL-based language model.\n\n\n\n\nType\nDetails\n\n\n\n\nmodel\ntyping.Callable\na pre-trained transformers model\n\n\n\n\nsource\n\n\nAgent.forward\n\n Agent.forward (input_ids:typing.Annotated[torch.Tensor,{'__torchtyping__'\n                :True,'details':('batch_size','seq_len',),'cls_name':'Tens\n                orType'}], attention_mask:Optional[Annotated[torch.Tensor,\n                {'__torchtyping__':True,'details':('batch_size,seq_len',),\n                'cls_name':'TensorType'}]]=None)\n\nsummary\n\n\nAgent Objective\nEquation 2 in the paper https://arxiv.org/abs/2203.02155\n\\(\\begin{aligned} \\operatorname{objective~}(\\phi)= & E_{(x, y) \\sim D_{\\pi_\\phi^{\\mathrm{RL}}}}\\left[r_\\theta(x, y)-\\beta \\log \\left(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\ & \\gamma E_{x \\sim D_{\\text {pretrain }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\\end{aligned}\\)\n\nsource\n\n\nAgentObjective\n\n AgentObjective (model:Callable, sft_model:Callable,\n                 reward_model:Callable, gamma:float, beta:float)\n\nAgent objective.\n\n\n\n\nType\nDetails\n\n\n\n\nmodel\ntyping.Callable\nthe language model\n\n\nsft_model\ntyping.Callable\nthe reference model\n\n\nreward_model\ntyping.Callable\nthe reward model\n\n\ngamma\nfloat\n\n\n\nbeta\nfloat\n\n\n\n\n\nsource\n\n\nAgentObjective.forward\n\n AgentObjective.forward (input_ids:typing.Annotated[torch.Tensor,{'__torch\n                         typing__':True,'details':('batch_size','seq_len',\n                         ),'cls_name':'TensorType'}], attention_mask:typin\n                         g.Annotated[torch.Tensor,{'__torchtyping__':True,\n                         'details':('batch_size','seq_len',),'cls_name':'T\n                         ensorType'}])\n\nCalculate the objective value given the input ids and attention mask.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninput_ids\ntyping.Annotated[torch.Tensor, {‘torchtyping’: True, ‘details’: (‘batch_size’, ‘seq_len’,), ‘cls_name’: ‘TensorType’}]\n\n\n\nattention_mask\ntyping.Annotated[torch.Tensor, {‘torchtyping’: True, ‘details’: (‘batch_size’, ‘seq_len’,), ‘cls_name’: ‘TensorType’}]\n\n\n\nReturns\ntyping.Annotated[torch.Tensor, {‘torchtyping’: True, ‘details’: (1,), ‘cls_name’: ‘TensorType’}]\nA scalar objective value"
  },
  {
    "objectID": "dataset.html",
    "href": "dataset.html",
    "title": "Dataset",
    "section": "",
    "text": "Dataset for Reward Model\n\nsource\n\n\nPairDataset\n\n PairDataset (dataset:Iterable, tokenizer:Callable, max_length:int=1024)\n\nPairwise dataset for train reward model.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\ntyping.Iterable\n\nA dataset\n\n\ntokenizer\ntyping.Callable\n\nThe tokenizer of the reward model\n\n\nmax_length\nint\n1024\nMax context length of the reward model\n\n\n\n\n\nDataset for PPO Agent\n\nsource\n\n\nPromptDataset\n\n PromptDataset (dataset:Iterable, tokenizer:Callable, max_length:int=1024)\n\nDataset for train RL-based language model.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\ntyping.Iterable\n\nA dataset\n\n\ntokenizer\ntyping.Callable\n\nThe tokenizer of the language model\n\n\nmax_length\nint\n1024\nMax context length of the language model"
  }
]